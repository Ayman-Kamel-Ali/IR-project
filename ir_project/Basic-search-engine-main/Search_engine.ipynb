{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from natsort import natsorted\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(file):\n",
    "    if 'txt' in file:\n",
    "        with open(f'Articles/'+file, 'r') as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file in os.listdir('Articles'):\n",
    "    documents.append(read_files(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Phase $:-$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_docs = []\n",
    "for document in documents:\n",
    "    token_docs.append(word_tokenize(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_docs =[[ps.stem(term) for term in token] for token in token_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second phase $:-$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement function to do all steps in first phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(doc):\n",
    "    token_docs = word_tokenize(doc)\n",
    "    ps = PorterStemmer()\n",
    "    prepared_doc = [ps.stem(term) for term in token_docs]\n",
    "    return prepared_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the file no.\n",
    "fileno = 1\n",
    "\n",
    "# Initialize the dictionary.\n",
    "pos_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open files.\n",
    "file_names = natsorted(os.listdir(\"Articles\"))\n",
    "print(file_names)\n",
    "# For every file.\n",
    "for file_name in file_names:\n",
    "\n",
    "    # Read file contents.\n",
    "    with open(f'Articles/{file_name}', 'r') as f:\n",
    "        doc = f.read()\n",
    "    # preprocess doc\n",
    "    final_token_list = preprocessing(doc)\n",
    "\n",
    "    # For position and term in the tokens.\n",
    "    for pos, term in enumerate(final_token_list):\n",
    "        # print(pos, '-->' ,term)\n",
    "        \n",
    "        # If term already exists in the positional index dictionary.\n",
    "        if term in pos_index:\n",
    "                \n",
    "            # Increment total freq by 1.\n",
    "            pos_index[term][0] = pos_index[term][0] + 1\n",
    "            \n",
    "            # Check if the term has existed in that DocID before.\n",
    "            if fileno in pos_index[term][1]:\n",
    "                pos_index[term][1][fileno].append(pos)\n",
    "                    \n",
    "            else:\n",
    "                pos_index[term][1][fileno] = [pos]\n",
    "\n",
    "        # If term does not exist in the positional index dictionary\n",
    "        else:\n",
    "            \n",
    "            # Initialize the list.\n",
    "            pos_index[term] = []\n",
    "            # The total frequency is 1.\n",
    "            pos_index[term].append(1)\n",
    "            # The postings list is initially empty.\n",
    "            pos_index[term].append({})     \n",
    "            # Add doc ID to postings list.\n",
    "            pos_index[term][1][fileno] = [pos]\n",
    "\n",
    "    # Increment the file no. counter for document ID mapping             \n",
    "    fileno += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### displays each term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allow users to write phrase query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phrase Query\n",
    "def put_query(q, display=1):\n",
    "    lis = [[] for i in range(10)]\n",
    "    q = preprocessing(q)\n",
    "    for term in q:\n",
    "\n",
    "        if term in pos_index.keys():\n",
    "            for key in pos_index[term][1].keys():\n",
    "            \n",
    "                if lis[key-1] != []:\n",
    "                    \n",
    "                    if lis[key-1][-1] == pos_index[term][1][key][0]-1:\n",
    "                        lis[key-1].append(pos_index[term][1][key][0])\n",
    "                else:\n",
    "                    lis[key-1].append(pos_index[term][1][key][0])\n",
    "    positions = []\n",
    "    if display==1:\n",
    "        for pos, list in enumerate(lis, start=1):\n",
    "            if len(list) == len(q):\n",
    "                positions.append('document '+str(pos))\n",
    "        return positions\n",
    "    else:\n",
    "        for pos, list in enumerate(lis, start=1):\n",
    "            if len(list) == len(q):\n",
    "                positions.append('doc'+str(pos))\n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'antony brutus caeser cleopatra mercy worser'\n",
    "put_query(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third phase $:-$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "files = os.listdir('Articles')\n",
    "for file in range(1, 11):\n",
    "    documents.append(\" \".join(preprocessing(read_files(str(file)+'.txt'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_terms = []\n",
    "for doc in documents:\n",
    "    for term in doc.split():\n",
    "        all_terms.append(term)\n",
    "all_terms = set(all_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency\n",
    "$$ tf = \\frac{number of times the term appears in a document} {total number of words in the document}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(document):\n",
    "    wordDict = dict.fromkeys(all_terms, 0)\n",
    "    for word in document.split():\n",
    "        wordDict[word] += 1\n",
    "    return wordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = pd.DataFrame(get_tf(documents[0]).values(), index=get_tf(documents[0]).keys())\n",
    "for i in range(1, len(documents)):\n",
    "    tf[i] = get_tf(documents[i]).values()\n",
    "tf.columns = ['doc'+str(i) for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted tf(1+ log tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_tf(x):\n",
    "    if x > 0:\n",
    "        return math.log10(x) + 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tf = tf.copy()\n",
    "for i in range(0, len(documents)):\n",
    "    w_tf['doc'+str(i+1)] = tf['doc'+str(i+1)].apply(weighted_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency\n",
    "$$ idf = \\frac{number of the documents in the corups} {number of documents in the corups contain the term}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame(columns=['df', 'idf'])\n",
    "for i in range(len(tf)):\n",
    "    in_term = w_tf.iloc[i].values.sum()\n",
    "\n",
    "    tdf.loc[i, 'df'] = in_term\n",
    "\n",
    "    tdf.loc[i, 'idf'] = math.log10(10 / (float(in_term)))\n",
    "\n",
    "tdf.index=w_tf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = w_tf.multiply(tdf['idf'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_len(col):\n",
    "    return np.sqrt(tf_idf[col].apply(lambda x: x**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len = pd.DataFrame()\n",
    "for col in tf_idf.columns:\n",
    "    doc_len.loc[0, col+'_length']= get_doc_len(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len['doc1_length'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized TF.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_tf_idf(col, x):\n",
    "    try:\n",
    "        return x / doc_len[col+'_length'].values[0]\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tf_idf = pd.DataFrame()\n",
    "for col in tf_idf.columns:\n",
    "    norm_tf_idf[col] = tf_idf[col].apply(lambda x : get_norm_tf_idf(col, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tf_idf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_tf(x):\n",
    "    try:\n",
    "        return math.log10(x)+1\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_query(q):\n",
    "    docs_found = put_query(q, 2)\n",
    "    if docs_found == []:\n",
    "        return \"Not Fount\"\n",
    "    new_q = preprocessing(q)\n",
    "    query = pd.DataFrame(index=norm_tf_idf.index)\n",
    "    query['tf'] = [1 if x in new_q else 0 for x in list(norm_tf_idf.index)]\n",
    "    query['w_tf'] = query['tf'].apply(lambda x : get_w_tf(x))\n",
    "    product = norm_tf_idf.multiply(query['w_tf'], axis=0)\n",
    "    query['idf'] = tdf['idf'] * query['w_tf']\n",
    "    query['tf_idf'] = query['w_tf'] * query['idf']\n",
    "    query['normalized'] = 0\n",
    "    for i in range(len(query)):\n",
    "        query['normalized'].iloc[i] = float(query['idf'].iloc[i]) / math.sqrt(sum(query['idf'].values**2))\n",
    "    print('Query Details')\n",
    "    print(query.loc[new_q])\n",
    "    product2 = product.multiply(query['normalized'], axis=0)\n",
    "    scores = {}\n",
    "    for col in put_query(q, 2):\n",
    "            scores[col] = product2[col].sum()\n",
    "    product_result = product2[list(scores.keys())].loc[new_q]\n",
    "    print()\n",
    "    print('Product (query*matched doc)')\n",
    "    print(product_result)\n",
    "    print()\n",
    "    print('product sum')\n",
    "    print(product_result.sum())\n",
    "    print()\n",
    "    print('Query Length')\n",
    "    q_len = math.sqrt(sum([x**2 for x in query['idf'].loc[new_q]]))\n",
    "    print(q_len)\n",
    "    print()\n",
    "    print('Cosine Simliarity')\n",
    "    print(product_result.sum())\n",
    "    print()\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    print('Returned docs')\n",
    "    for typle in sorted_scores:\n",
    "        print(typle[0], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_query('antony brutus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b35187bd8e81c46a491b0701dacbc6455fb6bb49fda2ea4057c13cf77d84decb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
